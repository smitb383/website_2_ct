<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Instruction Sets</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" media="screen" href="../base.css" />
    <link rel="stylesheet" type="text/css" media="screen" href="../main.css" />
    <link rel="stylesheet" type="text/css" media="screen" href="openF.css" />
    <link rel="stylesheet" type="text/css" media="screen" href="../nav2.css" />

</head>

<body id="project">
    <div class="navigation">
        <div class="logoTopNav">
            <a href="../../index.html"> <img src="../BS LogoWhite.png"></a>
        </div>
        <div class="pages">
            <a href="../../index.html" class="nav">Home</a>
            <a href="../work/work.html" class="nav">Work</a>
            <a href="../about/about.html" class=" nav">About</a>
            <!-- <a href="../contact/contact.html" class="nav">Contact</a> -->
        </div>
    </div>
    <div class="content">
        <h1>Ambient Emotion</h1>
        <!-- <h1>Emotional Space</h1> -->
        <h2>4D computational “aura” mapping </h2>
        <div class="video">
            <video autoplay muted loop>
                <source src="OpenVK1_cropped.mp4" type="video/mp4">
            </video>
        </div>

        <p id="introP">
            After discovering Vokaturi, a machine learning software which estimates
            emotions in speech, I became interested in investigating the accuracy of this
            technology to
            collect and quantify abstract human data, such as internal emotional states. Specifically, I
            wanted to
            <span id="highlight">explore the gaps between the
                technology and
                the human experience and visually interface these calculations.</span>

            My goal
            creating
            these 4D
            computational “aura” maps was to <span id="highlight">visually translate the technology’s prescription
                of
                meaning to
                ambient, nonsensical
                space and sound </span>into an interactive piece.
        </p>


        <h3>Interactive Installations </h3>
        <section class="video">
            <div class="sideBySideVid">
                <video autoplay muted loop>
                    <source src="openVK_2_trim.mp4" type="video/mp4">
                </video>
            </div>
            <div class="sideBySideVid">
                <video autoplay muted loop>
                    <source src="openVK_3_trim.mp4" type="video/mp4">
                </video>
            </div>
        </section>

        <p id="caption">
            Video clips showing generative-maps being drawn to the screen in real-tine when I set up my application
            with
            a kinnect and monitor in different environments.
        </p>
        <p id="introP">Using OpenFrameworks, <span id="highlight">I incorporated the Vokaturi software for real-time
                calculations </span>across
            five basic
            emotions:
            neutrality, happiness, sadness, anger, and fear. I <span id="highlight">created a
                colour blending algorithm </span>so that the probabilities across the various emotions correspond to
            mixed
            RGB
            values. <span id="highlight">I then used Xbox Kinect to map these colours back onto the environments</span>
            in which the
            auditory
            data is collected from, creating a emotional map of this calculation over time in
            environments.</p>


        <h3> Color Calculations </h3>
        <div class="color">
            <section class="explained">

                <div class="process">
                    <img src="COLOR_C ALCULATIONS-100.jpg">
                </div>
                <div class="textContainer">
                    <p id="caption">
                        This RGB value is based on the original color of each emotion that is detected and blended
                        according to the strength of the
                        probability
                        estimated by the Vokaturi neural network.
                    </p>
                </div>

            </section>

            <section class="explained">
                <div class="textContainer">
                    <p id="caption">
                        To inform the viewer of the calculations generating these color maps, I
                        printed
                        the name of each emotion in its assigned colour
                        to the screen. I updating these emotion names and calculations in real-time to the screen
                        and
                        mapped
                        their
                        screen-visibility
                        based on the strength of their calculed probabilities.</p>

                </div>
                <div class="process">
                    <video autoplay muted loop>
                        <source src="openVK_2_readingZoom.mp4" id="myVid" type="video/mp4" alt="to inform">
                    </video>
                </div>

            </section>

            <section class="explained">
                <div class="process">
                    <video autoplay muted loop>
                        <source src="openVK_process_fade4.mp4" id="myVid" type="video/mp4">
                    </video>
                </div>
                <div class="textContainer">
                    <p id="caption">
                        I lastly updated my canvas with an nearly transparent background every
                        several
                        frames
                        to create
                        a lingering/fading effect of the color maps to refelct the time of emotions expressed,
                        or
                        rather
                        computated, as
                        a measure of brightness in the screen. </p>

                </div>
            </section>
        </div>
        <video autoplay muted loop id="myVideo">
            <source src="../splash_video.mp4" type="video/mp4">

            <video autoplay muted loop id="myVideo">
                <source src="splash_video.mp4" type="video/mp4">

            </video>
    </div>
</body>
<script src="openF.js"></script>

</html>